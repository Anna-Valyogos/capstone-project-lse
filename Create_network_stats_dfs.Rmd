---
title: "Create_network_stat_dfs"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo= FALSE}
library(sna)
library(tsna)
library(ndtv)
library(dplyr)
library(magrittr)
library(igraph)
library(fastDummies)
```

```{r Read_in_data}
edges_df <- read.csv('./data/econlit_edges_simple.csv',
                     row.names = 1)
nodes_df <- read.csv('./data/econlit_nodes.csv',
                     row.names = 1)
pols_df_jel_dummy <- read.csv('./data/econlit_author_article.csv',
                              row.names = 1)
```

```{r Recode_author_names_to_numeric}
# make vertex ids ie nodes (currently author names) numeric
nodes_df['node_id'] = seq(1:nrow(nodes_df))
my_map = setNames(c(nodes_df$node_id), c(nodes_df$author))
edges_df['author_1_id'] <- my_map[edges_df$author.1]
edges_df['author_2_id'] <- my_map[edges_df$author.2]
# code authors based on dictionary used above
pols_df_jel_dummy['author_id'] <- my_map[pols_df_jel_dummy$author]
# rename gender column in nodes df
nodes_df = rename(nodes_df, gender = gender_fn_95)
```

```{r Reorganize dfs}
# reorganize columns
edges_df = edges_df[c('collab_start_year', 'pub_year', 'author.1', 'author.2', 'author_1_id', 'author_2_id')]
head(edges_df)
# create first publication year variable for nodes
nodes_df['first_pub_year'] = substr(nodes_df$first_pub_date, 1, 4)
nodes_df['terminus'] = 2020
# reorganize columns to be able to pass directly as metadata to igraph creation (graph_from_data_frame)
nodes_df = nodes_df[c("node_id", "author", "gender", "first_pub_date", "first_pub_year", "terminus")]  
```


```{r Loop for all 51 years -- yearly momentary stats}

network_stats_df = data.frame(matrix(ncol = 24, nrow = 0))
colnames(network_stats_df) = c('year', 
                               'group_size_m', 'group_size_f', 'group_size_diff',
                               'group_share_m', 'group_share_f', 'group_share_diff',
                               'avg_num_coauths', 'avg_num_coauths_m', 'avg_num_coauths_f', 'avg_num_coauths_diff',
                               'avg_num_ties', 'avg_num_ties_m', 'avg_num_ties_f', 'avg_num_ties_diff',
                               'avg_clust', 'avg_clust_m', 'avg_clust_f', 'avg_clust_diff',
                               'avg_tie_s', 'avg_tie_s_m', 'avg_tie_s_f', 'avg_tie_s_diff',
                               'assortativity_of_netw')
years = sort(unique(edges_df$pub_year))
for (year_i in years){
  static_edges = edges_df[edges_df$pub_year == year_i, c('author_1_id', 'author_2_id')]
  static_edges_no_loops = static_edges[!is.na(static_edges$author_2_id), ]
  unique_auths_year = unique(c(static_edges$author_1_id, static_edges$author_2_id))
  static_verts = nodes_df[nodes_df$node_id %in% unique_auths_year, ]
  static_igraph = graph_from_data_frame(static_edges_no_loops,  
                                        directed = FALSE, 
                                        vertices = static_verts) 
  
  # GROUP SIZE (per gender)
  # calculate how many male and female authors formed the network (and NA)
  group_size_f = length(which(V(static_igraph)$gender=='female'))
  group_size_m = length(which(V(static_igraph)$gender=='male'))
  #group_size_NA = length(which(V(static_igraph)$gender==''))
  group_size_diff = group_size_f - group_size_m
  
  group_share_f = group_size_f / gorder(static_igraph)
  group_share_m = group_size_m / gorder(static_igraph)
  group_share_diff = group_share_f - group_share_m
  
  # UNIQUE CO-AUTHORS
  # calculate average number of unique co-authors for whole graph, females, males, and the gender difference
  number_of_neighbours = igraph::neighborhood.size(static_igraph) - 1 # -1 because all nodes count themselves too
  avg_number_of_neighbours = mean(igraph::neighborhood.size(static_igraph) - 1)
  avg_number_of_neighbours_f = mean(number_of_neighbours[which(V(static_igraph)$gender=='female')])
  avg_number_of_neighbours_m = mean(number_of_neighbours[which(V(static_igraph)$gender=='male')])
  avg_number_of_neighbours_diff = avg_number_of_neighbours_f - avg_number_of_neighbours_m
  
  # NUMBER OF TIES - for Currarini et al. 2009 test section
  num_ties = degree(static_igraph)
  avg_num_ties = mean(num_ties, na.rm = T)
  avg_num_ties_f = mean(num_ties[which(V(static_igraph)$gender=='female')], na.rm = T)
  avg_num_ties_m = mean(num_ties[which(V(static_igraph)$gender=='male')], na.rm = T)
  avg_num_ties_diff = avg_num_ties_f - avg_num_ties_m 
  
  # CLUSTERING
  average_clustering = transitivity(static_igraph)  
  avg_clust_f = mean(transitivity(static_igraph, 
                                  type = 'localundirected',
                                  vids = V(static_igraph)[which(V(static_igraph)$gender=='female')]),
                     na.rm = TRUE)
  avg_clust_m = mean(transitivity(static_igraph, 
                                  type = 'localundirected',
                                  vids = V(static_igraph)[which(V(static_igraph)$gender=='male')]),
                     na.rm = TRUE)
  avg_clust_diff = avg_clust_f - avg_clust_m

  # TIE STRENGTH
  tie_strength =  degree(static_igraph) / (igraph::neighborhood.size(static_igraph) - 1)
  avg_tie_strength = mean(tie_strength, na.rm = T)
  avg_tie_strength_f = mean(tie_strength[which(V(static_igraph)$gender=='female')], na.rm = T)
  avg_tie_strength_m = mean(tie_strength[which(V(static_igraph)$gender=='male')], na.rm = T)
  avg_tie_strength_diff = avg_tie_strength_f - avg_tie_strength_m 
  
  # ASSORTATIVITY COEFFICIENT  - for homophily section
  assort_coef <- igraph::assortativity_nominal(static_igraph, 
                                               types = as.integer(as.factor(V(static_igraph)$gender)), 
                                               directed = FALSE)
  
  # Add statistics to data frame
  new_row = c(year_i,
              group_size_m, group_size_f, group_size_diff,
              group_share_m, group_share_f, group_share_diff,
              avg_number_of_neighbours, avg_number_of_neighbours_m, avg_number_of_neighbours_f, avg_number_of_neighbours_diff,
              avg_num_ties, avg_num_ties_m, avg_num_ties_f, avg_num_ties_diff,
              average_clustering, avg_clust_m, avg_clust_f, avg_clust_diff,
              avg_tie_strength, avg_tie_strength_m, avg_tie_strength_f, avg_tie_strength_diff,
              assort_coef)

  network_stats_df[nrow(network_stats_df)+1,] <- new_row 

  print(paste('year', year_i, 'done'))
}

write.csv(network_stats_df,
          './data/network_stats_yearly.csv') 
```

```{r Create data for POLS anaylsis}
# code authors based on dictionary used above
pols_df_jel_dummy['author_id'] <- my_map[pols_df_jel_dummy$author]

# assign centrality measure to appropriate rows (based on author and year)
pols_df_jel_dummy_merged = pols_df_jel_dummy[c('author_id', 'pub_year')]

years = sort(unique(edges_df$pub_year))

for (year_i in years){
  static_edges = edges_df[edges_df$pub_year == year_i, c('author_1_id', 'author_2_id')]
  static_edges_no_loops = static_edges[!is.na(static_edges$author_2_id), ]
  unique_auths_year = unique(c(static_edges$author_1_id, static_edges$author_2_id))
  static_verts = nodes_df[nodes_df$node_id %in% unique_auths_year, ]
  static_igraph = graph_from_data_frame(static_edges_no_loops,  
                                        directed = FALSE, 
                                        vertices = static_verts) 
  
  # DEGREE of each author
  degrees = igraph::degree(static_igraph)
  degrees_df = as.data.frame(degrees)
  degrees_df['year'] = year_i # add year column
  degrees_df <- cbind(node_id = rownames(degrees_df), degrees_df) # make node_id into column from index
    
  
  # NEIGHBOURS (UNIQUE) 
  neighbor_nodes = igraph::neighborhood.size(static_igraph)
  neighbor_nodes_df = as.data.frame(neighbor_nodes - 1)
  neighbor_nodes_df['year'] = year_i 
  neighbor_nodes_df <- cbind(node_id = rownames(neighbor_nodes_df), neighbor_nodes_df) # unnamed but in the same order

  # Merge statistics
  net_stats_to_merge = cbind(degrees_df, neighbor_nodes_df$`neighbor_nodes - 1`)
  names(net_stats_to_merge)[names(net_stats_to_merge) == "neighbor_nodes_df$`neighbor_nodes - 1`"] <- 'neighbours'
  
  # STRENGTH OF TIES
  net_stats_to_merge['tie_strength'] = net_stats_to_merge$degrees / net_stats_to_merge$neighbours
  
  # CLUSTERING for each author
  clusterings = transitivity(static_igraph, type = 'local') 
  clusterings_df = as.data.frame(clusterings)
  # merge with degrees, neighbours and tie strengths (based on order)
  net_stats_to_merge <- cbind(net_stats_to_merge, clusterings_df)
  
  # merge net_stats_to_merge to pols_df together
  # merge degrees with correct authors (those active in year_i) - left outer join
  pols_df_jel_dummy_merged = merge(pols_df_jel_dummy_merged, 
                                   net_stats_to_merge, 
                                   by.x = c('author_id', 'pub_year'),
                                   by.y = c('node_id', 'year'),
                                   all.x = TRUE)
  
  # rename degree and tie_strength columns to avoid duplicate column names
  names(pols_df_jel_dummy_merged)[names(pols_df_jel_dummy_merged) == "degrees"] <- 
    paste0('degree_', toString(year_i))
  names(pols_df_jel_dummy_merged)[names(pols_df_jel_dummy_merged) == "neighbours"] <- 
    paste0('neighbours_', toString(year_i))
  names(pols_df_jel_dummy_merged)[names(pols_df_jel_dummy_merged) == "tie_strength"] <- 
    paste0('tie_strength_', toString(year_i))
  names(pols_df_jel_dummy_merged)[names(pols_df_jel_dummy_merged) == "clusterings"] <- 
    paste0('clusterings_', toString(year_i))
  
  print(paste('year', year_i, 'done'))
  
}

# sum across degrees, tie strengths, clusterings - to get a single number for all rows

# change all NaNs to NAs with method dispatch
is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.nan))
pols_df_jel_dummy_merged[is.nan(pols_df_jel_dummy_merged)] <- NA

# first get subdfs
degree_cols <- sapply(names(pols_df_jel_dummy_merged), function(x) grep("degree", x)) == 1
degree_cols_df <- pols_df_jel_dummy_merged[!is.na(degree_cols)]
degree_cols_df <- cbind(degree_cols_df, pols_df_jel_dummy_merged[, c('author_id', 'pub_year')])

neighbours_cols <- sapply(names(pols_df_jel_dummy_merged), function(x) grep("neighbours", x)) == 1
neighbours_cols_df <- pols_df_jel_dummy_merged[!is.na(neighbours_cols)]
neighbours_cols_df <- cbind(neighbours_cols_df, pols_df_jel_dummy_merged[, c('author_id', 'pub_year')])

tie_strength_cols <- sapply(names(pols_df_jel_dummy_merged), function(x) grep("tie", x)) == 1
tie_strength_cols_df <- pols_df_jel_dummy_merged[!is.na(tie_strength_cols)]
tie_strength_cols_df <- cbind(tie_strength_cols_df, pols_df_jel_dummy_merged[, c('author_id', 'pub_year')])

clustering_cols <- sapply(names(pols_df_jel_dummy_merged), function(x) grep("clusterings", x)) == 1
clustering_cols_df <- pols_df_jel_dummy_merged[!is.na(clustering_cols)]
clustering_cols_df <- cbind(clustering_cols_df, pols_df_jel_dummy_merged[, c('author_id', 'pub_year')])

# create a single degree, tie strength, clustering for each row (dependent variables in POLS)
pols_df_jel_dummy_merged['degree'] = rowSums(select(degree_cols_df, -c('author_id', 'pub_year')),
                                             na.rm=TRUE)

pols_df_jel_dummy_merged['neighbours'] = rowSums(select(neighbours_cols_df, -c('author_id', 'pub_year')),
                                                 na.rm=TRUE)

pols_df_jel_dummy_merged['tie_strength'] = rowSums(select(tie_strength_cols_df, -c('author_id', 'pub_year')),
                                                   na.rm=TRUE) * (NA ^ !(rowSums(!is.na(select(tie_strength_cols_df, -c('author_id', 'pub_year')))) > 0 )) 

pols_df_jel_dummy_merged['clustering'] = rowSums(select(clustering_cols_df, -c('author_id', 'pub_year')), na.rm = TRUE) * (NA ^  !(rowSums(!is.na(select(clustering_cols_df, -c('author_id', 'pub_year')))) > 0 ))


# select relevant columns only
pols_df_jel_dummy_merged_small = select(pols_df_jel_dummy_merged, c("author_id", "pub_year", 
                                                                    "degree", 'neighbours', 'tie_strength', 'clustering'))

# write only network stats to csv
write.csv(pols_df_jel_dummy_merged_small, 
          './data/per_author_net_stats_yearly.csv')

# merge back the JEL codes, experience, based on year and author
pols_df_jel_dummy_merged_full = merge(pols_df_jel_dummy_merged_small, pols_df_jel_dummy, 
                                      by = c('author_id', 'pub_year'))

# recode gender variable as female dummy
# asign NA when gender is empty
pols_df_jel_dummy_merged_full [pols_df_jel_dummy_merged_full $gender_fn_95 == "", 'gender_fn_95'] <- NA 
# create dummies
pols_df_jel_dummy_merged_full <- dummy_columns(pols_df_jel_dummy_merged_full,
                                               select_columns = "gender_fn_95",
                                               ignore_na = TRUE)
# rename columns to female and male
names(pols_df_jel_dummy_merged_full)[names(pols_df_jel_dummy_merged_full) == "gender_fn_95_female"] <- "female"
names(pols_df_jel_dummy_merged_full)[names(pols_df_jel_dummy_merged_full) == "gender_fn_95_male"] <- "male"

# save the data
write.csv(pols_df_jel_dummy_merged_full,
          './data/per_author_net_stats_yearly_full.csv')

```

```{r Create data for POLS anaylsis - 5 years rolling}

# Repeat the dataframe creation from yearly version.

# assign centrality measure to appropriate rows (based on author and year)
pols_df_jel_dummy_5yroll = pols_df_jel_dummy[c('author_id', 'pub_year')]

years = sort(unique(edges_df$pub_year))
for (year_i in years){
  static_edges = edges_df[edges_df$pub_year <= year_i & edges_df$pub_year >= year_i - 4, c('author_1_id', 'author_2_id')]
  static_edges_no_loops = static_edges[!is.na(static_edges$author_2_id), ]
  unique_auths_year = unique(c(static_edges$author_1_id, static_edges$author_2_id))
  static_verts = nodes_df[nodes_df$node_id %in% unique_auths_year, ]
  static_igraph = graph_from_data_frame(static_edges_no_loops,  
                                        directed = FALSE, 
                                        vertices = static_verts) 
  
  # DEGREE of each author
  degrees = igraph::degree(static_igraph)
  degrees_df = as.data.frame(degrees)
  degrees_df['year'] = year_i # add column about year
  degrees_df <- cbind(node_id = rownames(degrees_df), degrees_df) # make node_id into column from index
    
  
  # NEIGHBOURS (UNIQUE) 
  neighbor_nodes = igraph::neighborhood.size(static_igraph)
  neighbor_nodes_df = as.data.frame(neighbor_nodes - 1)
  neighbor_nodes_df['year'] = year_i 
  neighbor_nodes_df <- cbind(node_id = rownames(neighbor_nodes_df), neighbor_nodes_df) # unnamed but in the same order
  
  # Merge statistics in
  net_stats_to_merge = cbind(degrees_df, neighbor_nodes_df$`neighbor_nodes - 1`)
  names(net_stats_to_merge)[names(net_stats_to_merge) == "neighbor_nodes_df$`neighbor_nodes - 1`"] <- 'neighbours'
  
  # STRENGTH OF TIES
  # tie strength calculation: # total edges (ie. degree) / # total neighbours
  net_stats_to_merge['tie_strength'] = net_stats_to_merge$degrees / net_stats_to_merge$neighbours
  
  
  # CLUSTERING for each author
  clusterings = transitivity(static_igraph, type = 'local') 
  clusterings_df = as.data.frame(clusterings)
  # merge with degrees, neighbours and tie strengths (based on order)
  net_stats_to_merge <- cbind(net_stats_to_merge, clusterings_df)

  # merge net_stats_to_merge to pols_df together
  # merge degrees with correct authors (those active in year_i) - left outer join
  pols_df_jel_dummy_5yroll = merge(pols_df_jel_dummy_5yroll, 
                                   net_stats_to_merge, 
                                   by.x = c('author_id', 'pub_year'),
                                   by.y = c('node_id', 'year'),
                                   all.x = TRUE)
  
  # rename degree and tie_strength columns to avoid duplicate column names
  names(pols_df_jel_dummy_5yroll)[names(pols_df_jel_dummy_5yroll) == "degrees"] <- 
    paste0('degree_', toString(year_i))
  names(pols_df_jel_dummy_5yroll)[names(pols_df_jel_dummy_5yroll) == "neighbours"] <- 
    paste0('neighbours_', toString(year_i))
  names(pols_df_jel_dummy_5yroll)[names(pols_df_jel_dummy_5yroll) == "tie_strength"] <- 
    paste0('tie_strength_', toString(year_i))
  names(pols_df_jel_dummy_5yroll)[names(pols_df_jel_dummy_5yroll) == "clusterings"] <- 
    paste0('clusterings_', toString(year_i))
  
  print(paste('year', year_i, 'done'))
  
}

# sum across degrees, tie strengths, clusterings - to get a single number for all rows

# change all NaNs to NAs with method dispatch
is.nan.data.frame <- function(x)
do.call(cbind, lapply(x, is.nan))
pols_df_jel_dummy_5yroll[is.nan(pols_df_jel_dummy_5yroll)] <- NA

# first get subdfs
degree_cols <- sapply(names(pols_df_jel_dummy_5yroll), function(x) grep("degree", x)) == 1
degree_cols_df <- pols_df_jel_dummy_5yroll[!is.na(degree_cols)]
degree_cols_df <- cbind(degree_cols_df, pols_df_jel_dummy_5yroll[, c('author_id', 'pub_year')])

neighbours_cols <- sapply(names(pols_df_jel_dummy_5yroll), function(x) grep("neighbours", x)) == 1
neighbours_cols_df <- pols_df_jel_dummy_5yroll[!is.na(neighbours_cols)]
neighbours_cols_df <- cbind(neighbours_cols_df, pols_df_jel_dummy_5yroll[, c('author_id', 'pub_year')])

tie_strength_cols <- sapply(names(pols_df_jel_dummy_5yroll), function(x) grep("tie", x)) == 1
tie_strength_cols_df <- pols_df_jel_dummy_5yroll[!is.na(tie_strength_cols)]
tie_strength_cols_df <- cbind(tie_strength_cols_df, pols_df_jel_dummy_5yroll[, c('author_id', 'pub_year')])

clustering_cols <- sapply(names(pols_df_jel_dummy_5yroll), function(x) grep("clusterings", x)) == 1
clustering_cols_df <- pols_df_jel_dummy_5yroll[!is.na(clustering_cols)]
clustering_cols_df <- cbind(clustering_cols_df, pols_df_jel_dummy_5yroll[, c('author_id', 'pub_year')])

# create a single degree, tie strength, clustering for each row (dependent variables in POLS)
pols_df_jel_dummy_5yroll['degree'] = rowSums(select(degree_cols_df, -c('author_id', 'pub_year')),
                                             na.rm=TRUE)

pols_df_jel_dummy_5yroll['neighbours'] = rowSums(select(neighbours_cols_df, -c('author_id', 'pub_year')),
                                                 na.rm=TRUE)

pols_df_jel_dummy_5yroll['tie_strength'] = rowSums(select(tie_strength_cols_df, -c('author_id', 'pub_year')),
                                                   na.rm=TRUE) * (NA ^ !(rowSums(!is.na(select(tie_strength_cols_df, -c('author_id', 'pub_year')))) > 0 )) 

pols_df_jel_dummy_5yroll['clustering'] = rowSums(select(clustering_cols_df, -c('author_id', 'pub_year')), na.rm = TRUE) * (NA ^  !(rowSums(!is.na(select(clustering_cols_df, -c('author_id', 'pub_year')))) > 0 ))


# select relevant columns only
pols_df_jel_dummy_5yroll_small = select(pols_df_jel_dummy_5yroll, c("author_id", "pub_year", 
                                                                     "degree", 'neighbours', 'tie_strength', 'clustering'))

# write only network stats to csv
write.csv(pols_df_jel_dummy_5yroll_small, 
          './data/per_author_net_stats_5yroll.csv')

# merge back the JEL codes, experience, based on year and author
pols_df_jel_dummy_5yroll_full = merge(pols_df_jel_dummy_5yroll_small, pols_df_jel_dummy, 
                                      by = c('author_id', 'pub_year'))

# recode gender variable as female dummy
# asign NA when gender is empty
 pols_df_jel_dummy_5yroll_full [ pols_df_jel_dummy_5yroll_full $gender_fn_95 == "", 'gender_fn_95'] <- NA 
# create dummies
 pols_df_jel_dummy_5yroll_full <- dummy_columns( pols_df_jel_dummy_5yroll_full,
                                               select_columns = "gender_fn_95",
                                               ignore_na = TRUE)
# rename columns to female and male
names( pols_df_jel_dummy_5yroll_full)[names( pols_df_jel_dummy_5yroll_full) == "gender_fn_95_female"] <- "female"
names( pols_df_jel_dummy_5yroll_full)[names( pols_df_jel_dummy_5yroll_full) == "gender_fn_95_male"] <- "male"

write.csv( pols_df_jel_dummy_5yroll_full,
          './data/per_author_net_stats_5yroll_full')
```

